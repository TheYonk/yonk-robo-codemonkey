# RoboMonkey Daemon Configuration
# This file configures the background daemon for automatic indexing and embedding

# =============================================================================
# LLM Configuration (Dual Model Setup)
# =============================================================================
# Two models: "deep" for complex tasks, "small" for simple tasks
#
# Supported providers:
#   - "ollama"  : Local Ollama server (default, no API key needed)
#   - "openai"  : OpenAI API or any OpenAI-compatible endpoint
#   - "vllm"    : vLLM server with /v1/completions endpoint
#
# API Key Configuration:
#   For cloud providers, you can either:
#   1. Set api_key directly in this config file
#   2. Use environment variables (recommended for security):
#      - OPENAI_API_KEY for "openai" provider
#      - VLLM_API_KEY for "vllm" provider
#
# -----------------------------------------------------------------------------
# CLOUD PROVIDER EXAMPLES (uncomment and modify as needed)
# -----------------------------------------------------------------------------
#
# OpenAI:
#   deep:
#     provider: "openai"
#     model: "gpt-4o"
#     base_url: "https://api.openai.com"
#     # api_key: "sk-..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#   small:
#     provider: "openai"
#     model: "gpt-4o-mini"
#     base_url: "https://api.openai.com"
#     temperature: 0.3
#     max_tokens: 1000
#
# Azure OpenAI:
#   deep:
#     provider: "openai"
#     model: "gpt-4o"  # Your deployment name
#     base_url: "https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT"
#     api_key: "your-azure-key"
#     temperature: 0.3
#     max_tokens: 4000
#
# Together.ai:
#   deep:
#     provider: "openai"
#     model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
#     base_url: "https://api.together.xyz"
#     # api_key: "..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#
# Groq:
#   deep:
#     provider: "openai"
#     model: "llama-3.3-70b-versatile"
#     base_url: "https://api.groq.com/openai"
#     # api_key: "gsk_..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#
# -----------------------------------------------------------------------------

llm:
  # Deep model: Complex code analysis, feature context, comprehensive reviews
  deep:
    provider: "ollama"
    model: "qwen3-coder:30b"
    base_url: "http://localhost:11434"
    # api_key: ""  # Not needed for ollama; for openai/vllm set here or use env var
    temperature: 0.3
    max_tokens: 4000

  # Small model: Quick summaries, classifications, simple questions
  small:
    provider: "ollama"
    model: "phi3.5:3.8b"
    base_url: "http://localhost:11434"
    # api_key: ""  # Not needed for ollama; for openai/vllm set here or use env var
    temperature: 0.3
    max_tokens: 1000

# =============================================================================
# Database Configuration
# =============================================================================
database:
  # Control schema DSN (REQUIRED)
  control_dsn: "postgresql://postgres:postgres@localhost:5433/robomonkey"

  # Schema prefix for per-repo isolation
  schema_prefix: "robomonkey_"

  # Connection pool size
  pool_size: 10

# Embeddings Configuration
embeddings:
  # Enable/disable automatic embedding generation
  enabled: true
  
  # Backfill embeddings for existing chunks on startup
  backfill_on_startup: true
  
  # Provider: "ollama" or "vllm"
  provider: "ollama"
  
  # Model configuration
  model: "snowflake-arctic-embed2:latest"
  dimension: 1024
  # Reduced to 4000 chars to stay safely under 8192 token limit
  # (minified JS/dense data can have very poor character:token ratios ~1:2)
  max_chunk_length: 4000
  batch_size: 100
  
  # Provider-specific settings
  ollama:
    base_url: "http://localhost:11434"
  
  vllm:
    base_url: "http://localhost:8000"
    api_key: "local-key"

# Auto-Summary Generation
summaries:
  # Enable/disable automatic summary generation
  enabled: true

  # How often to check for entities needing summaries (minutes: 1-1440)
  # Default: 60 (check every hour)
  # Reduced to 10 minutes for more responsive summary generation
  check_interval_minutes: 10

  # Generate summaries immediately after indexing (not just on schedule)
  generate_on_index: true

  # LLM provider for summary generation: "ollama" or "vllm"
  provider: "ollama"

  # Model to use for generating summaries
  # Using Qwen3 Coder 30B Instruct for better code understanding
  model: "mdq100/Qwen3-Coder-30B-A3B-Instruct:30b"

  # LLM endpoint
  base_url: "http://localhost:11434"

  # Batch size for summary generation (1-100)
  batch_size: 10

# Document Validity Scoring
doc_validity:
  # Enable/disable document validity scoring
  enabled: true

  # How often to check for documents needing validation (minutes: 10-1440)
  # Default: 120 (every 2 hours)
  check_interval_minutes: 60

  # Score weights (should sum to 1.0)
  reference_weight: 0.55    # Weight for code reference validity
  embedding_weight: 0.30    # Weight for semantic similarity
  freshness_weight: 0.15    # Weight for freshness (age since last validated)

  # Thresholds
  stale_threshold: 50       # Score below which doc is considered stale
  warning_threshold: 70     # Score below which to show warning

  # LLM validation (optional, expensive)
  use_llm_validation: false
  llm_provider: "ollama"
  llm_model: "mdq100/Qwen3-Coder-30B-A3B-Instruct:30b"
  llm_base_url: "http://localhost:11434"

  # Performance
  batch_size: 20
  max_references_per_doc: 100

  # Semantic validation (behavioral claim verification)
  semantic_validation_enabled: true
  semantic_check_interval_minutes: 60   # Every hour
  max_claims_per_doc: 30
  claim_min_confidence: 0.7
  semantic_batch_size: 15
  semantic_min_structural_score: 60    # Only validate docs with decent structural scores

# Job Workers Configuration
workers:
  # Global concurrency limits
  global_max_concurrent: 4
  max_concurrent_per_repo: 2

  # Worker counts by type
  reindex_workers: 2
  embed_workers: 2
  docs_workers: 1

  # Polling interval (seconds)
  poll_interval_sec: 5

  # Legacy fields (for backward compatibility)
  count: 2
  enabled_job_types:
    - EMBED_REPO
    - EMBED_MISSING
    - INDEX_REPO
    - WATCH_REPO

# Repository Watching
watching:
  # Enable file system watching for indexed repos
  enabled: true

  # Debounce interval (seconds) - wait this long after last change
  debounce_seconds: 2

  # Patterns to ignore (in addition to .gitignore)
  ignore_patterns:
    - "*.pyc"
    - "__pycache__"
    - ".git"
    - "node_modules"
    - ".venv"

  # File extensions to watch for code changes
  code_extensions:
    - ".py"
    - ".js"
    - ".jsx"
    - ".ts"
    - ".tsx"
    - ".go"
    - ".java"
    - ".ejs"      # EJS templates
    - ".hbs"      # Handlebars templates
    - ".handlebars"
    - ".html"     # HTML with embedded scripts
    - ".htm"
    - ".vue"      # Vue.js components
    - ".svelte"   # Svelte components
    - ".astro"    # Astro components

  # File extensions to watch for documentation changes
  doc_extensions:
    - ".md"       # Markdown
    - ".rst"      # ReStructuredText
    - ".adoc"     # AsciiDoc

# Daemon Health & Monitoring
monitoring:
  # Heartbeat interval (seconds)
  heartbeat_interval: 30

  # Consider daemon dead after this many seconds without heartbeat
  dead_threshold: 120

  # Log level: DEBUG, INFO, WARNING, ERROR
  log_level: "INFO"

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Development Mode Settings
dev_mode:
  # Enable development mode features
  enabled: false
  
  # Auto-reload on code changes
  auto_reload: false
  
  # Verbose logging
  verbose: false
