# RoboMonkey Daemon Configuration
# This file configures the background daemon for automatic indexing and embedding

# =============================================================================
# LLM Configuration (Dual Model Setup)
# =============================================================================
# Two models: "deep" for complex tasks, "small" for simple tasks
#
# Supported providers:
#   - "ollama"  : Local Ollama server (default, no API key needed)
#   - "openai"  : OpenAI API or any OpenAI-compatible endpoint
#   - "vllm"    : vLLM server with /v1/completions endpoint
#
# API Key Configuration:
#   For cloud providers, you can either:
#   1. Set api_key directly in this config file
#   2. Use environment variables (recommended for security):
#      - OPENAI_API_KEY for "openai" provider
#      - VLLM_API_KEY for "vllm" provider
#
# -----------------------------------------------------------------------------
# CLOUD PROVIDER EXAMPLES (uncomment and modify as needed)
# -----------------------------------------------------------------------------
#
# OpenAI:
#   deep:
#     provider: "openai"
#     model: "gpt-4o"
#     base_url: "https://api.openai.com"
#     # api_key: "sk-..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#   small:
#     provider: "openai"
#     model: "gpt-4o-mini"
#     base_url: "https://api.openai.com"
#     temperature: 0.3
#     max_tokens: 1000
#
# Azure OpenAI:
#   deep:
#     provider: "openai"
#     model: "gpt-4o"  # Your deployment name
#     base_url: "https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT"
#     api_key: "your-azure-key"
#     temperature: 0.3
#     max_tokens: 4000
#
# Together.ai:
#   deep:
#     provider: "openai"
#     model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
#     base_url: "https://api.together.xyz"
#     # api_key: "..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#
# Groq:
#   deep:
#     provider: "openai"
#     model: "llama-3.3-70b-versatile"
#     base_url: "https://api.groq.com/openai"
#     # api_key: "gsk_..."  # Or set OPENAI_API_KEY env var
#     temperature: 0.3
#     max_tokens: 4000
#
# -----------------------------------------------------------------------------

llm:
  # Deep model: Complex code analysis, feature context, comprehensive reviews
  deep:
    provider: "ollama"
    model: "qwen3-coder:30b"
    base_url: "http://localhost:11434"
    # api_key: ""  # Not needed for ollama; for openai/vllm set here or use env var
    temperature: 0.3
    max_tokens: 4000

  # Small model: Quick summaries, classifications, simple questions
  small:
    provider: "ollama"
    model: "phi3.5:3.8b"
    base_url: "http://localhost:11434"
    # api_key: ""  # Not needed for ollama; for openai/vllm set here or use env var
    temperature: 0.3
    max_tokens: 1000

# =============================================================================
# Database Configuration
# =============================================================================
database:
  # Control schema DSN (REQUIRED)
  control_dsn: "postgresql://postgres:postgres@localhost:5433/robomonkey"

  # Schema prefix for per-repo isolation
  schema_prefix: "robomonkey_"

  # Connection pool size
  pool_size: 10

# =============================================================================
# Embeddings Configuration
# =============================================================================
# Supports three providers:
#   - "ollama"  : Local Ollama server (/api/embeddings)
#   - "vllm"    : Local vLLM server (/v1/embeddings)
#   - "openai"  : OpenAI-compatible API (local service OR cloud OpenAI)
#
# -----------------------------------------------------------------------------
# CLOUD OPENAI EMBEDDINGS EXAMPLE
# -----------------------------------------------------------------------------
# To use cloud OpenAI embeddings (text-embedding-3-small/large):
#
#   embeddings:
#     provider: "openai"
#     model: "text-embedding-3-small"
#     dimension: 1536
#     openai:
#       base_url: "https://api.openai.com"
#       api_key: "sk-..."  # Or set OPENAI_API_KEY env var
#
# -----------------------------------------------------------------------------
# LOCAL EMBEDDING SERVICE EXAMPLE (Docker)
# -----------------------------------------------------------------------------
# The Docker install includes a local embedding service (sentence-transformers):
#
#   embeddings:
#     provider: "openai"
#     model: "all-mpnet-base-v2"
#     dimension: 768
#     openai:
#       base_url: "http://embeddings:8082"
#       api_key: ""  # No auth needed for local service
#
# -----------------------------------------------------------------------------

embeddings:
  # Enable/disable automatic embedding generation
  enabled: true

  # Backfill embeddings for existing chunks on startup
  backfill_on_startup: true

  # Provider: "ollama", "vllm", or "openai"
  # Use "openai" for the local embedding service or any OpenAI-compatible API
  provider: "ollama"

  # Model configuration
  model: "snowflake-arctic-embed2:latest"
  dimension: 1024
  # Reduced to 4000 chars to stay safely under 8192 token limit
  # (minified JS/dense data can have very poor character:token ratios ~1:2)
  max_chunk_length: 4000
  batch_size: 100

  # Provider-specific settings
  ollama:
    base_url: "http://localhost:11434"

  vllm:
    base_url: "http://localhost:8000"
    api_key: "local-key"

  # OpenAI-compatible API (for local embedding service or cloud providers)
  openai:
    base_url: "http://localhost:8082"  # Default: local embedding service
    api_key: ""  # Empty for local service, set for cloud APIs

  # Auto-rebuild vector indexes after embedding jobs
  # Rebuilds IVFFlat/HNSW indexes when significant data changes occur
  auto_rebuild_indexes: true

  # Minimum change rate (0-1) to trigger rebuild
  # 0.20 = rebuild if 20% or more embeddings added/changed
  # Set to 0 to always rebuild, 1 to never rebuild
  rebuild_change_threshold: 0.20

  # Index type to use on rebuild: "ivfflat" or "hnsw"
  # IVFFlat: Fast builds, good for frequently changing data
  # HNSW: Better recall, slower builds, good for stable data > 100k rows
  rebuild_index_type: "ivfflat"

  # HNSW-specific parameters (only used when rebuild_index_type: "hnsw")
  rebuild_hnsw_m: 16              # Max connections per layer (4-64)
  rebuild_hnsw_ef_construction: 64  # Build-time search width (16-512)

# Auto-Summary Generation
summaries:
  # Enable/disable automatic summary generation
  enabled: true

  # Read-only settings: prevent overwriting specific data types
  # Useful when loading pre-generated summaries from a backup
  read_only:
    summaries: false        # Global - skip all summary generation
    file_summaries: false   # Skip file summary generation
    symbol_summaries: false # Skip symbol summary generation
    module_summaries: false # Skip module summary generation
    embeddings: false       # Skip embedding regeneration

  # How often to check for entities needing summaries (minutes: 1-1440)
  # Default: 60 (check every hour)
  check_interval_minutes: 60

  # Generate summaries immediately after indexing (not just on schedule)
  generate_on_index: true

  # LLM provider for summary generation: "ollama" or "vllm"
  provider: "ollama"

  # Model to use for generating summaries
  # Using Qwen3 Coder 30B Instruct for better code understanding
  model: "mdq100/Qwen3-Coder-30B-A3B-Instruct:30b"

  # LLM endpoint
  base_url: "http://localhost:11434"

  # Batch size for summary generation (1-100)
  batch_size: 10

# Document Validity Scoring
doc_validity:
  # Enable/disable document validity scoring
  enabled: true

  # How often to check for documents needing validation (minutes: 10-1440)
  # Default: 120 (every 2 hours)
  check_interval_minutes: 60

  # Score weights (should sum to 1.0)
  reference_weight: 0.55    # Weight for code reference validity
  embedding_weight: 0.30    # Weight for semantic similarity
  freshness_weight: 0.15    # Weight for freshness (age since last validated)

  # Thresholds
  stale_threshold: 50       # Score below which doc is considered stale
  warning_threshold: 70     # Score below which to show warning

  # LLM validation (optional, expensive)
  use_llm_validation: false
  llm_provider: "ollama"
  llm_model: "mdq100/Qwen3-Coder-30B-A3B-Instruct:30b"
  llm_base_url: "http://localhost:11434"

  # Performance
  batch_size: 20
  max_references_per_doc: 100

  # Semantic validation (behavioral claim verification)
  semantic_validation_enabled: true
  semantic_check_interval_minutes: 60   # Every hour
  max_claims_per_doc: 30
  claim_min_confidence: 0.7
  semantic_batch_size: 15
  semantic_min_structural_score: 60    # Only validate docs with decent structural scores

# =============================================================================
# Job Workers Configuration
# =============================================================================
# Controls how the daemon processes background jobs (indexing, embedding, etc.)
#
# Processing Modes:
#   - "single"   : One worker processes all jobs sequentially (low resource usage)
#   - "per_repo" : Dedicated worker per active repo, up to max_workers
#   - "pool"     : Thread pool claims jobs from queue (default, most flexible)
#
# For machines with limited resources, use "single" mode.
# For powerful machines (8+ cores, 32GB+ RAM), use "pool" with higher workers.
# =============================================================================

workers:
  # Processing mode: "single", "per_repo", or "pool"
  mode: "pool"

  # Global concurrency limits
  max_workers: 4              # Maximum concurrent job workers
  max_concurrent_per_repo: 2  # Prevent one repo from hogging all workers

  # Per job-type limits (only in "pool" mode)
  # Limits how many concurrent jobs of each type can run
  job_type_limits:
    FULL_INDEX: 2       # Max concurrent indexing jobs
    EMBED_MISSING: 3    # Max concurrent embedding jobs
    SUMMARIZE_MISSING: 2 # Max concurrent summary jobs
    SUMMARIZE_FILES: 2
    SUMMARIZE_SYMBOLS: 2
    DOCS_SCAN: 1

  # Polling interval (seconds) - how often to check for new jobs
  poll_interval_sec: 5

  # Job timeout (seconds) - kill jobs that run longer than this
  job_timeout_sec: 3600  # 1 hour

  # Retry settings
  max_retries: 3
  retry_backoff_multiplier: 2  # Exponential backoff: 1min, 2min, 4min, etc.

  # Legacy fields (for backward compatibility)
  global_max_concurrent: 4
  reindex_workers: 2
  embed_workers: 2
  docs_workers: 1
  count: 2
  enabled_job_types:
    - EMBED_REPO
    - EMBED_MISSING
    - INDEX_REPO
    - WATCH_REPO

# Repository Watching
watching:
  # Enable file system watching for indexed repos
  enabled: true

  # Debounce interval (seconds) - wait this long after last change
  debounce_seconds: 2

  # Patterns to ignore (in addition to .gitignore)
  ignore_patterns:
    - "*.pyc"
    - "__pycache__"
    - ".git"
    - "node_modules"
    - ".venv"

  # File extensions to watch for code changes
  code_extensions:
    - ".py"
    - ".js"
    - ".jsx"
    - ".ts"
    - ".tsx"
    - ".go"
    - ".java"
    - ".ejs"      # EJS templates
    - ".hbs"      # Handlebars templates
    - ".handlebars"
    - ".html"     # HTML with embedded scripts
    - ".htm"
    - ".vue"      # Vue.js components
    - ".svelte"   # Svelte components
    - ".astro"    # Astro components

  # File extensions to watch for documentation changes
  doc_extensions:
    - ".md"       # Markdown
    - ".rst"      # ReStructuredText
    - ".adoc"     # AsciiDoc

# Daemon Health & Monitoring
monitoring:
  # Heartbeat interval (seconds)
  heartbeat_interval: 30

  # Consider daemon dead after this many seconds without heartbeat
  dead_threshold: 120

  # Log level: DEBUG, INFO, WARNING, ERROR
  log_level: "INFO"

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Development Mode Settings
dev_mode:
  # Enable development mode features
  enabled: false
  
  # Auto-reload on code changes
  auto_reload: false
  
  # Verbose logging
  verbose: false
