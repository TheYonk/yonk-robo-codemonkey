# Database Configuration
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/robomonkey

# Embeddings Configuration
EMBEDDINGS_PROVIDER=ollama
EMBEDDINGS_MODEL=snowflake-arctic-embed2:latest
EMBEDDINGS_BASE_URL=http://localhost:11434
EMBEDDINGS_DIMENSION=1024
MAX_CHUNK_LENGTH=8192
EMBEDDING_BATCH_SIZE=100

# Alternative embedding models:
# Ollama:
#   - nomic-embed-text (768 dimensions, 2048 token limit)
#   - snowflake-arctic-embed2:latest (1024 dimensions, 8192 token limit) - RECOMMENDED
#   - mxbai-embed-large (1024 dimensions, 512 token limit)
# OpenAI:
#   - text-embedding-3-small (1536 dimensions) - cheap, good quality
#   - text-embedding-3-large (3072 dimensions) - best quality

# vLLM Configuration (alternative to Ollama)
VLLM_BASE_URL=http://localhost:8000
VLLM_API_KEY=local-key

# =============================================================================
# LLM Configuration (for daemon summaries and analysis)
# =============================================================================
# Note: LLM config is primarily in config/robomonkey-daemon.yaml
# These are fallback/override environment variables

# For Ollama (local - default):
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434
# LLM_DEEP_MODEL=qwen2.5-coder:14b
# LLM_SMALL_MODEL=qwen2.5-coder:7b
# LLM_MODEL=qwen2.5-coder:14b  # Used by Docker to auto-pull on startup

# For OpenAI (cloud):
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# LLM_BASE_URL=https://api.openai.com
# Deep models: gpt-5.2-codex (best), gpt-5.2, gpt-5.2-pro, gpt-5, gpt-4.1
# LLM_DEEP_MODEL=gpt-5.2-codex
# Small models: gpt-5-mini (recommended), gpt-5-nano (fastest)
# LLM_SMALL_MODEL=gpt-5-mini

# =============================================================================
# Source Code Directory (Docker mode)
# =============================================================================
# For Docker deployments, specify where your source code lives on the host.
#
# Option 1: Single directory (simple)
#   Set SOURCE_DIR to mount one parent directory containing your projects:
#   SOURCE_DIR=/home/user/projects
#   Then index with: robomonkey index --repo /source/myproject --name myproject
#
# Option 2: Multiple directories (recommended)
#   Use the helper script to manage multiple source directories:
#   ./scripts/manage-sources.sh add /path/to/code1 repo1
#   ./scripts/manage-sources.sh add /path/to/code2 repo2
#   ./scripts/manage-sources.sh apply
#
# The script generates docker-compose.override.yml with proper volume mounts.
SOURCE_DIR=

# Repository Scanning (native/non-Docker mode)
REPO_ROOT=/path/to/your/repo
IGNORE_FILE=.gitignore
WATCH_MODE=false

# Search Parameters
VECTOR_TOP_K=30
FTS_TOP_K=30
FINAL_TOP_K=12
CONTEXT_BUDGET_TOKENS=12000
GRAPH_DEPTH=2

# Schema Isolation (one schema per repo)
SCHEMA_PREFIX=robomonkey_
USE_SCHEMAS=true

# MCP Server Configuration
# Default repository for MCP queries (optional)
# If set, MCP tools will use this repo when no explicit repo is specified
# Example: DEFAULT_REPO=myproject
DEFAULT_REPO=
